---
title: 2025-03-02
date: 2025-03-02
slug: blog-post-slug
tags:
  - 毕业设计
  - Serverless
categories:
  - 日记
description: 指导记录
summary: 
draft: true
---
## 前一阶段总结 

### 基础理论学习

1. **Serverless计算概念**：Serverless计算是一种新兴云计算范式，可提供细粒度、事件驱动服务。它将应用程序从底层服务器和操作系统中抽象出来，开发者无需管理和维护服务器等基础设施，只需专注于编写应用逻辑。平台负责资源的自动管理、弹性扩展以及按使用量计费，如AWS Lambda、Azure Functions等产品。 
2. **关键特性** 
	- **敏捷可扩展性**：能根据工作负载自动快速调整资源，应对流量高峰和低谷，无需人工干预。 
	- **按需付费定价**：用户仅为实际使用的资源付费，避免资源闲置浪费，降低成本。 
	- **易于使用**：简化开发流程，减少运维工作，使开发者能更高效地部署和运行应用。 
3. **在机器学习推理中的应用**：适用于机器学习推理任务，可将机器学习代码打包成轻量级无服务器函数，借助平台弹性处理推理请求。但存在一些挑战，如模型加载时间长、资源管理复杂等，需要针对性优化。 
4. **相关开发技术**：涉及多种开源平台，如Openwhisk、Openfaas、Kserve等，需学习其架构、运行机制、函数调度策略、资源管理方式等。同时，要掌握与机器学习推理相关的技术，如常见模型结构、推理算法，以及模型在无服务器环境下运行的特殊需求等。 
### 重点论文设计思路与重要算法 
1. **《Pre-Warming is Not Enough: Accelerating Serverless Inference With Opportunistic Pre-Loading》** 
	- **设计思路**：针对无服务器计算中机器学习推理函数冷启动时模型加载延迟高的问题，提出InstaInfer系统。利用平台已有的空闲容器和GPU实例，机会性地预加载机器学习模型和库，避免额外资源开销，实现即时推理。 
	- **关键算法**
		- **函数调用预测算法**：基于平台固有预测模型预测函数调用到达时间，通过设置预加载和卸载概率阈值，确定函数的预加载和卸载时机。采用滑动窗口机制，捕捉函数调用的时间变化，提高预测准确性。 
		- **延迟感知装箱策略**：用于为函数选择合适的容器进行预加载，将容器和函数视为背包问题中的箱子和物品，通过动态规划计算最大延迟节省值，优化函数到容器的分配，充分利用空闲内存，提高整体加速效果。 
2. **《SMSS: Stateful Model Serving in Metaverse With Serverless Computing and GPU Sharing》** 
	- **设计思路**：为解决在元宇宙中基于无服务器计算部署机器学习推理模型时，无服务器计算的无状态性和GPU资源共享支持不足的问题，提出SMSS系统。通过引入基于日志的工作流运行时支持和两层GPU共享机制，实现有状态的模型推理服务。
	- **重要算法**：无特别突出的算法，主要是系统设计方面的创新。如通过控制平面管理系统节点和任务分配；利用对象存储层实现灵活的状态管理；设计日志存储系统用于函数恢复；定义服务器less运行时的几种状态，提供两层GPU共享机制，根据任务运行状态分配推理工作到GPU容器，提高GPU利用率。 
3. **《AsyFunc: A High-Performance and Resource-Efficient Serverless Inference System via Asymmetric Functions》** 
	- **设计思路**：鉴于现有服务器less推理平台在处理突发工作负载时，因模型加载过程耗时且资源需求大导致的资源效率低问题，提出AsyFunc系统。通过将深度学习模型内部层的异质性利用起来，提出非对称函数概念，即Body Function加载完整模型处理稳定需求，Shadow Function加载部分资源敏感层应对突发需求。 
	- **关键算法**
		- **模型级缩放启发式算法（MLS）**：根据历史请求信息和实例状态，通过计算现有实例能处理的最大请求率，决定是否扩展或收缩Body Function实例，并选择最优的CPU核心配置，以满足稳定工作负载需求并优化资源效率。 
		- **层级别缩放启发式算法（LLS）**：在突发情况下，根据请求队列长度、实例状态和模型元数据，确定Shadow Function的配置（包括CPU核心数和加载的层块），以满足突发需求并最小化资源消耗。同时，采用自适应调度算法，根据实例状态和请求的预计服务时间，将请求调度到最合适的实例，提高处理效率和满足服务级别目标（SLO）。 
4. **《HarmonyBatch: Batching multi-SLO DNN Inference with Heterogeneous Serverless Functions》** 
	- **设计思路**：为解决现有服务器less DNN推理仅优化单个应用的批处理请求，无法处理多SLO（服务级别目标）DNN推理且未充分利用异构函数资源的问题，提出HarmonyBatch框架。通过构建DNN推理性能和成本模型，设计两阶段合并策略，实现对多SLO DNN推理请求的高效批处理和资源配置。 
	- **关键算法**
		- **两阶段合并策略**：第一阶段将原本部署在CPU函数上的应用组尽可能合并到GPU函数上，根据请求到达率的膝点作为合并阈值，判断是否进行合并以配置更高效的GPU函数；第二阶段对部署在GPU函数上的相邻组基于SLO进行合并，以增加合并组的请求到达率，降低整体成本。 
		- **funcProvision函数资源配置算法**：基于推导的两个定理，采用二分搜索方法快速确定CPU函数的vCPU核心数和GPU函数的批大小，以实现成本高效的函数资源配置，在满足SLO的同时最小化推理预算。 
5. **《FaaSwap: SLO-Aware, GPU-Efficient Serverless Inference via Model Swapping》** 
	- **设计思路**：针对现有服务器less平台对GPU支持不足，无法满足低延迟推理需求的问题，提出FaaSwap平台。通过采用后期绑定策略，将模型保持在主内存中，根据请求动态交换到GPU上执行，实现高效的GPU共享和低延迟推理。 
	- **关键算法**
		- **请求调度算法**：根据模型的带宽密集程度将模型分为重模型和轻模型，优先通过NVLink进行重模型的GPU到GPU交换，避免PCIe带宽竞争，减少并发模型交换的干扰，降低推理延迟。 
		- **模型驱逐策略**：倾向于缓存重模型在GPU中，驱逐轻模型，减少主机到GPU的数据传输和干扰。在每个组内采用最近最少使用（LRU）策略确定驱逐顺序，结合请求调度策略，最小化并发模型执行之间的干扰。 
		- **SLO感知请求排队策略**：根据函数满足SLO的可能性对请求进行优先级排序，将函数分为高优先级和低优先级两组，分别维护两个队列。通过量化所需请求数（RRC）来衡量函数满足SLO的可能性，动态调整函数在队列中的优先级，最大化满足SLO的函数数量。
6. **《INFless: A Native Serverless System for Low-Latency, High-Throughput Inference》**
	- 重新研读后，深入理解了其针对现有无服务器平台在机器学习推理方面的不足，设计出 INFless 这一特定领域无服务器平台的思路。
	- 在系统设计上，核心组件非均匀缩放引擎能灵活分配资源和工作负载；内置非均匀批处理机制，将批处理集成到平台，且各实例批大小和资源配额可不同。通过公式 $r_{up}=\left\lfloor\frac{1}{t_{exec}}\right\rfloor × b$ 和 $r_{low}=\left\lceil\frac{1}{t_{slo}-t_{exec}}\right\rceil × b$ 控制请求到达率，保证延迟服务水平目标（SLO）。
	- 关键算法：
		- 采用轻量级组合算子分析（COP）方法估计延迟，根据推理函数共享算子且执行时间由少数算子主导的特点，通过组合算子轮廓估计整体推理延迟。
		- 调度方面，利用贪心算法解决资源分配优化问题，以最小化资源使用并保证延迟 SLO。
		- 同时，提出长短期直方图（LSTH）策略减少冷启动率，通过跟踪长短周期空闲时间确定预热和保持活动窗口，公式为 $pre - warm=\gamma L_{pre warm}+(1 - \gamma)S_{prewarn}$ 和 $keep - alive=\gamma L_{keepalive}+(1 - \gamma)S_{keepalive}$ 。
- **《ENOVA: Autoscaling towards Cost-effective and Stable Serverless LLM Serving》**：
	- 服务配置模块针对 LLM 任务和 GPU 设备的多样性，确定如 max_num_seqs、gpu_memory 等最优配置。例如通过公式 $max\_num\_seqs \approx n_{limit } × t_{limit }^{r}$ 确定最大序列数，根据 GPU 内存利用率 $m^{u}$ 与运行请求数 $n^{r}$ 的关系 $m^{u}=g\left(n^{r}\right)$ 及监控指标确定 gpu_memory。
	- 性能检测模块采用半监督学习方法，以变分自编码器（VAE）为基础模型，通过优化变分下界证据（ELBO）检测异常，根据 KL 散度和输入指标与重建指标的平均差（MD）调整服务配置。
### **代码学习成果**

- **INFless 和 ENOVA 源码学习**：阅读基于 openfaas 的 INFless 源码和基于原生 k8s 的 ENOVA 源码，对系统实现细节有了进一步认识。了解到 INFless 在 OpenFaaS 基础上修改和添加功能组件实现自身功能，ENOVA 则利用多集群和本地集群调度器实现 LLM 服务自动部署。
- **Knative 和 KServe 学习**：简单了解 Knative，熟悉 KServe 源码。在 KServe 源码学习中，掌握了 K8s 的自定义资源定义（CRD）、Helm 包管理器、基于 Kubebuilder 的 K8s operator 开发，以及 KServe 的组件基本结构、Python SDK 和 Python 推理程序，为后续基于相关技术开发无服务器机器学习推理系统奠定了基础。
## 后一阶段计划 

1. **理论学习深化**：继续深入学习 Serverless 计算及开发理论，对比不同 Serverless 框架的优势与局限，重点研究如何将所学理论更好地应用到毕业设计系统设计中，如在资源调度、服务部署等方面的应用。
2. **代码实践推进**：基于对 INFless、ENOVA、Knative 和 KServe 的学习，开始尝试在选定的开源 Serverless 平台上进行简单的功能开发实践，如构建基础的机器学习模型推理服务框架，初步实现模型加载和简单推理功能，为后续完整推理系统的构建做准备。
3. **实验设计准备**：结合毕业设计实验方案要求，开始准备实验环境搭建所需的资源，收集合适的数据集，确定用于实验的机器学习模型，规划实验步骤和预期实验结果，为开展实验研究、验证系统性能和优化算法做前期准备工作。

## 问题与建议

1. **存在问题**
    - **理论理解深度不足**：虽然学习了相关理论知识，但对 Serverless 计算与机器学习推理结合的一些复杂概念和技术细节理解还不够深入，例如在复杂场景下的资源调度算法原理和实际应用，可能影响后续系统设计和优化。
    - **代码整合困难**：在阅读多种框架源码后，发现将不同框架的优势整合到毕业设计系统中存在困难，各框架的设计理念和实现方式差异较大，难以快速找到有效的整合点。
    - **实验规划不完善**：目前实验规划仅处于初步阶段，在实验环境搭建、数据集选择和实验指标设定方面可能考虑不够周全，影响实验结果的准确性和有效性。
2. **改进建议**
    - **深入研究理论知识**：针对理解不深入的部分，查阅更多相关资料、研究论文，结合实际案例进行分析，或者与指导教师和同学讨论，加深对关键技术的理解。
    - **梳理框架差异**：对所学的各个框架进行详细对比分析，梳理它们在功能、架构、接口等方面的差异和共性，从毕业设计系统需求出发，制定合理的整合策略，逐步实现框架间的协同工作。
    - **完善实验规划**：参考更多相关研究的实验设计，与指导教师沟通，对实验环境、数据集和指标进行全面评估和完善，确保实验能够准确反映系统性能和优化效果，为毕业设计提供有力支撑。
